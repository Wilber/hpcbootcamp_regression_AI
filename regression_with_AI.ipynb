{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9ee069-ebd1-475e-a679-1330eb3df114",
   "metadata": {
    "tags": []
   },
   "source": [
    "## An Introduction to Linear Regression with Applications to AI\n",
    "\n",
    "### Motivation for Linear Regression\n",
    "- Linear regression allows us to investigate the relationship between two or more variables statistically\n",
    "- Basic introduction to supervised learning\n",
    "- Can be thought of as a building block of artificial neural networks (Perceptrons)\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "Upon successful completion of this session, you should be able to:\n",
    "-   Fit a Simple Linear Regression model to data and interpret model coefficients\n",
    "-   Build and train a neural network (one layer perceptron) to solve a regression problem, as an introduction to the concept of Artificial Neural Networks (ANNs)\n",
    "-   Build and train a deep neural network using Keras -- a high-level, user-friendly API for building and training deep learning models. \n",
    "\n",
    "### Simple Linear Regression: A Statistical Approach\n",
    "\n",
    "#### Model definition\n",
    "\n",
    "We start by defining a simple linear regression (SLR) statistical model: \n",
    "The goal of a SLR model is to investigate the relationship between the **response** and the **predictor** variables. \n",
    "\n",
    "Recall -- from high school algebra -- that the equation of a line describing a linear relation between $x$ and $y$ has the following algebraic form:\n",
    "\n",
    "$y = b + mx$\n",
    "\n",
    "where $m$ is the slope and $b$ is the y-intercept.\n",
    "\n",
    "The general form of the SLR model -- for predicting a quantitative response (dependent) $Y$ on the basis of a single predictor (independent) variable $X$ -- closely resembles the equation of a line shown above, such that:\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 X + \\epsilon$\n",
    "\n",
    "For an individual observation ($x_i, y_i$), the regression equation becomes:\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n",
    "\n",
    "Where:\n",
    "-   $\\beta_0$ is the is the population y-intercept,\n",
    "-   $\\beta_1$ is the population slope,\n",
    "-   $x_i$ is the *i*th (predictor/independent) observation, and\n",
    "-   $\\epsilon_i$ is the error or deviation of observation $y_i$ from the line $\\beta_0 + \\beta_1 x_i$,\n",
    "-   and $\\epsilon \\sim N (0, \\sigma^2)$\n",
    "\n",
    "Together, $\\beta_0$ and $\\beta_1$ are known as the (unknown) population model ***coefficients*** or ***parameters***.\n",
    "\n",
    "We use training data (or random sample) to produce estimates of the parameters -- $\\hat\\beta_0$ and $\\hat\\beta_1$ to describe the relation between $Y$ and $X$, **and make predictions of $\\hat{y_i}$ given $x_i$**. Estimation is typically by the method of Ordinary Least Squares (OLS). \n",
    "\n",
    "#### Errors (Loss)\n",
    "\n",
    "The predicted (fitted) value of $Y$ ($\\hat{y_i}$) based on the *i*th value of $X$ ($\\hat{x_i}$) is obtained by:\n",
    "\n",
    "$fit_i=\\hat{y_i} = \\hat\\beta_0 + \\hat\\beta_1 x_i$\n",
    "\n",
    "Then\n",
    "\n",
    "$res_i=\\epsilon_i = y_i - \\hat{y_i}$\n",
    "\n",
    "represents the *i*th residual -- this is the difference between the *i*th observed response value and the *i*th response value that is predicted by our linear model.\n",
    "\n",
    "Loss is a measure of the difference between the actual values and our predictions. This difference is defined by Residual sum of squares (RSS) :\n",
    "\n",
    "$RSS=\\sum_{i=1}^{n}(y_i - \\hat{y_i}){^2}$\n",
    "\n",
    "With linear regression, it's common to use mean squared error (MSE), calculated using the formula:\n",
    "\n",
    "\n",
    "$MSE  = \\frac{1}{n}\\sum_{i=1}^{n} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$\n",
    "\n",
    "Least square estimates of $\\beta_0$ and $\\beta_1$ are values of intercept and slope that minimize MSE.\n",
    "\n",
    "#### Model assumptions for simple linear regression\n",
    "\n",
    "In an ideal SLR model, we obtain sub-populations of responses, one for each value of the explanatory/predictor variable.\n",
    "\n",
    "The regression of the response variable $Y$ on the explanatory variable $X$ is a mathematical relationship between the **means** of these sub-populations and the explanatory variable.\n",
    "\n",
    "The simple linear regression model specifies that this relationship is a straight line function of the explanatory variable.\n",
    "\n",
    "The following model assumptions must hold to warrant fitting a SLR model to data:\n",
    "\n",
    "1.  **Normality**: there is a normally distributed sub-population of responses for each value of the explanatory variable.\n",
    "\n",
    "2.  **Linearity**: the means of the sub-populations fall on a straight line function of the explanatory variable.\n",
    "\n",
    "3.  **Constant variance**: the sub-population standard deviations are all equal (to $\\sigma$).\n",
    "\n",
    "4.  **Independence**: the selection of an observation from any of the sub-populations is independent of the selection of any other observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661d35e-7e58-44f6-a41c-6cc608494d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.regressionplots import abline_plot\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a40fd-7cfb-4c7e-aef3-0e4a857fea4c",
   "metadata": {},
   "source": [
    "#### Is there a statistically significant relationship between height and weight?\n",
    "Suppose we took a ***random*** sample from students at a large university and asked them about their **height** and **weight**. We want to determine and quantify the relationship between height and weight. The dataset can be found [here](https://online.stat.psu.edu/stat500/sites/stat500/files/data/university_ht_wt.TXT). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26639a36-4cdb-4443-83c8-93efbed445ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load data from file\n",
    "university_ht_wt = pd.read_csv('data/university_ht_wt.csv')   # read in the data file\n",
    "university_ht_wt.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c54a41-5d2c-454d-bb78-2ac4bc9e6931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "university_ht_wt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af22bf1-cb49-4235-8596-36e210568b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with empty values of weight\n",
    "#university_ht_wt['weight'].replace('', np.nan, inplace=True) #deprecated\n",
    "university_ht_wt['weight'].replace('', np.nan)\n",
    "university_ht_wt.dropna(subset=['weight'], inplace=True)\n",
    "university_ht_wt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af724e40-8202-4f54-802e-1ea8f847ffb9",
   "metadata": {},
   "source": [
    "When applying a statistical model, check that model assumptions are met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c8712-ca96-4731-b4fa-abb0b5b65bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=university_ht_wt, x=\"height\", y=\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593447ea-3c3c-4435-a614-1e728c1be604",
   "metadata": {},
   "source": [
    "We fit a SLR model -- we estimate parameters that minimize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad7bb1-73be-4a34-82af-861a136d7c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fit a SLR model to data \n",
    "fit = smf.ols('weight ~ height', data=university_ht_wt).fit()\n",
    "\n",
    "#Obtain predcicted (fits) values; and residuals (errors)\n",
    "university_ht_wt['predicted_wt'] = fit.predict(university_ht_wt['height'])\n",
    "university_ht_wt['residuals'] = fit.resid\n",
    "university_ht_wt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863f262-55f3-4364-8fe1-7e72657844e7",
   "metadata": {},
   "source": [
    "#### Research Question: Is height a significant linear predictor of weight?\n",
    "\n",
    "The regression model that describes the relationship between $weight$ and $height$ variables is:\n",
    "\n",
    "$weight = \\beta_0 + \\beta_1 \\cdot height + \\epsilon$\n",
    "\n",
    "The hypotheses we are testing are:\n",
    "\n",
    "$H_0: \\beta_1 = 0$\n",
    "\n",
    "$H_A: \\beta_1 \\neq 0$\n",
    "\n",
    "We compute a *t-statistic*, given by\n",
    "\n",
    "$t = \\frac{\\hat{\\beta_1} - 0} {SE(\\hat{\\beta_1)}}$\n",
    "\n",
    "which measures the number of standard deviations that $\\hat\\beta_1$ is away from 0.\n",
    "\n",
    "If there really is no relationship between $X$ and $Y$ , then we expect that the *t-statistic* will have a *t*-distribution with *n*âˆ’2 degrees of freedom.\n",
    "\n",
    "The *t*-distribution has a bell shape and for values of *n* greater than approximately 30 and is quite similar to the standard normal distribution. Consequently, it is a simple matter to compute the probability of observing any number equal to *\\|t\\|* or larger in absolute value, assuming $\\beta_1 = 0$. We call this probability the p-value. \n",
    "\n",
    "Roughly speaking, we interpret the p-value as follows: **a small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and the response.**\n",
    "\n",
    "We obtain the model summary from the previous fit of model to the (presumably random) sample from the population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956655e-a683-4a59-8598-090c2d2d632f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021a786-abc7-4724-bbc8-38c93ab467ab",
   "metadata": {},
   "source": [
    "The regression equation for this fit becomes: \n",
    "\n",
    "**$weight = -222.48 + 5.49 *height$**\n",
    "\n",
    "since the slope ($\\beta_1$) is 5.49, the intercept ($\\beta_0$) is -222.\n",
    "\n",
    "The test for the slope has a p-value of less than 0.001. Therefore, with a significance level of 5% (and even as low as 0.1%), we can conclude that there is enough evidence to suggest that height is a significant linear predictor of weight.\n",
    "\n",
    "Differently stated, **an increase of one inch in height is associated with -- on average -- an increase of 5.488 lbs in weight.**\n",
    "\n",
    "Does $\\beta_0$ have a meaningful interpretation?\n",
    "\n",
    "The intercept is -222. Therefore, when height is equal to 0 (an unlikely scenario), then a person's weight is predicted to be -222 pounds. It is also not possible for someone to have a height of 0 inches or weight of -222 pounds. Therefore, the intercept does not have a valid meaning.\n",
    "\n",
    "##### What's the (95%?) confidence interval for the population slope?\n",
    "\n",
    "A 95% confidence interval is defined as a range of values such that with 95% interval probability, the range will contain the true unknown value of the parameter.\n",
    "\n",
    "For linear regression, the 95% confidence interval for $\\beta_1$ approximately takes the form\n",
    "\n",
    "$\\hat\\beta_1 \\pm t_\\frac{\\alpha}{2} SE(\\hat\\beta_1)$\n",
    "\n",
    "That is, there is approximately a 95% chance that the interval will contain the true value of $\\beta_1$.\n",
    "\n",
    "In the case of the student height-weight data, the 95% confidence interval for $\\beta_1$ (and $\\beta_0$) can be obtained by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc746bb6-56a0-4329-8619-5a1796be6d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d215baa-5c47-4ee6-a6a8-47d812ef9e4d",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression Model\n",
    "Simple linear regression is a useful approach for predicting a response\n",
    "on the basis of a single predictor variable. However, in practice we\n",
    "often have more than one predictor.\n",
    "\n",
    "A multiple linear model takes the form:\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon$\n",
    "\n",
    "where $X_j$ represents the jth predictor and $\\beta_j$ quantifies the\n",
    "association between that variable and the response.\n",
    "\n",
    "We interpret $\\beta_j$ as the average effect on $Y$ of a one unit\n",
    "increase in $X_j$, **holding all other predictors fixed.**\n",
    "\n",
    "As was the case in the simple linear regression setting, the regression\n",
    "coefficients $\\beta_0, \\beta_1, ..., \\beta_p$ in the above equation are\n",
    "unknown, and must be estimated.\n",
    "\n",
    "Given estimates $\\hat\\beta_0, \\hat\\beta_1, ..., \\hat\\beta_p$, we can\n",
    "make predictions using the formula\n",
    "\n",
    "$\\hat{y} = \\hat\\beta_0 + \\hat\\beta_1x_1 + \\hat\\beta_2x_2 + ... + \\hat\\beta_px_p$\n",
    "\n",
    "The parameters are estimated using the same least squares approach that\n",
    "we saw in the context of simple linear regression, where we choose\n",
    "$\\beta_0, \\beta_1,...,\\beta_p$ to minimize the MSE\n",
    "\n",
    "\n",
    "**A tutorial on data analysis with multiple linear regression can be found [here](https://github.com/Wilber/hpcbootcamp_regression/blob/main/regression_multiple.ipynb)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf84eea-d653-4467-aa7b-0437cd544c49",
   "metadata": {},
   "source": [
    "## Regression with Perceptron\n",
    "We will construct a neural network corresponding to a SLR model. We will train the network, implementing the gradient descent method. \n",
    "\n",
    "\n",
    "### Model definition \n",
    "We will start to use notation commonly used in machine learning, and define a SLR model as: \n",
    "\n",
    "$\\hat{y} = wx + b,\\tag{1}$\n",
    "\n",
    "with the weigh $w$ corresponding to the slope $\\beta_{1}$, and the bias $b$ corresponding to the intercept $\\beta_{0}$ \n",
    "\n",
    "\n",
    "The simplest neural network model that describes the above relationship can be realized by using one **perceptron**. The **input** and **output** layers will have one **node** each ($x$ for input and $\\hat{y} = z$ for output):\n",
    "\n",
    "<img src=\"images/nn_model_linear_regression_simple.png\" style=\"width:400px;\">\n",
    "\n",
    "**Weight** ($w$) and **bias** ($b$) are the parameters that will get updated when you **train** the model. They are initialized to some random values or set to 0 and updated as the training progresses.\n",
    "\n",
    "**Weighted sum ($Z$)** is the weighted sum of the linear combinations of inputs, weights and bias, **before** the activation function is applied. We are not applying an activation function in this example, hence $Z$ is an estimate of $\\hat{y}$.\n",
    "\n",
    "For each training example ($x_i, y_i$), the prediction $\\hat{y_i}$ can be calculated as:\n",
    "\n",
    "\n",
    "$z_i =  w x_i + b,$\n",
    "\n",
    "$\\hat{y_i}  =  z_i,$\n",
    "\n",
    "\n",
    "where $i = 1, \\dots, m$.\n",
    "\n",
    "We can organise all training examples as a vector $X$ of size ($1 \\times m$) and perform scalar multiplication of $X$ ($1 \\times m$) by a scalar $w$, adding $b$, which will be broadcasted to a vector of size ($1 \\times m$):\n",
    "\n",
    "\n",
    "$Z   = w X + b,$\n",
    "\n",
    "$\\hat{Y} = Z,$\n",
    "\n",
    "\n",
    "This set of calculations is called **forward propagation**.\n",
    "\n",
    "Given the estimate $\\hat{Y}$, we can now measure the difference between the actual $y_i$ and the estimated $\\hat{y_i}$ to obtain the errors (recall residuals) for each training example with the **loss function**: \n",
    "\n",
    "$L\\left(w, b\\right)  = \\frac{1}{2}\\left(\\hat{y_i} - y_i\\right)^2$\n",
    "\n",
    "\n",
    "To compare the resulting vector of the predictions $\\hat{Y}$ ($1 \\times m$) with the vector $Y$ of original values $y^{(i)}$, you can take an average of the loss function values for each of the training examples:\n",
    "\n",
    "$\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y_i} - y_i\\right)^2$\n",
    "\n",
    "This function is called the sum of squares **cost function**. The aim is to optimize the cost function during the training, which will minimize the differences between original values $y_i$ and predicted values $\\hat{y_i}$.\n",
    "\n",
    "When your weights were just initialized with some random values, and no training was done yet, you can't expect good results. You need to calculate the adjustments for the weight and bias, minimizing the cost function. This process is called **backward propagation**. \n",
    "\n",
    "The method we will use to minimize the cost function is **gradient descent**, and we will attempt to identify parameter values that minimize the cost function by taking partial derivatives of the cost function. \n",
    "\n",
    "<img src=\"images/gradient_descent.png\" style=\"width:500px;\">\n",
    "\n",
    "We calculate partial derivatives as:\n",
    "\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L} }{ \\partial w } = \\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y_i} - y_i\\right)x_i,$\n",
    "\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L} }{ \\partial b } = \\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y_i} - y_i\\right)$\n",
    "\n",
    "We then update the parameters iteratively using the expressions:\n",
    "\n",
    "\n",
    "$w = w - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial w },$\n",
    "\n",
    "$b = b - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b },$\n",
    "\n",
    "where $\\alpha$ is the learning rate. Then repeat the process until the cost function stops decreasing.\n",
    "\n",
    "The general **methodology** to build a neural network is to:\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Implement forward propagation (calculate the perceptron output),\n",
    "    - Implement backward propagation (to get the required corrections for the parameters),\n",
    "    - Update parameters.\n",
    "4. Make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564722f7-6ab9-49f7-b0ba-e5319277dac0",
   "metadata": {},
   "source": [
    "### A Neural Network Model for Linear Regression\n",
    "\n",
    "#### Define the Neural Network Structure\n",
    "\n",
    "We will use a [Kaggle dataset](https://www.kaggle.com/code/devzohaib/simple-linear-regression/notebook) (saved in a file `data/tvmarketing.csv`), to determine if there a relationship between `TV` advertising budget and the product `Sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf46c92-65d1-4e42-94e8-51f617056b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "path = \"data/tvmarketing.csv\"\n",
    "\n",
    "adv = pd.read_csv(path)\n",
    "\n",
    "adv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d9d80-8977-49ae-9fd2-5c5187010747",
   "metadata": {},
   "source": [
    "And add plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982165f-c6d3-4c01-93f5-6e52ecda7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv.plot(x='TV', y='Sales', kind='scatter', c='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44dc13-e50c-4619-8fb8-c982c2b13474",
   "metadata": {},
   "source": [
    "The fields `TV` and `Sales` have different units. To make gradient descent algorithm efficient, you needed to normalize each of them: subtract the mean value of the array from each of the elements in the array and divide them by the standard deviation.\n",
    "\n",
    "Column-wise normalization of the dataset can be done for all of the fields at once and is implemented in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ec44b-995b-4e26-853f-839051539208",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_norm = (adv - np.mean(adv, axis = 0))/np.std(adv, axis = 0)\n",
    "\n",
    "adv_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d413d5a-53d6-4828-aaff-7e58e6545cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_norm.plot(x='TV', y='Sales', kind='scatter', c='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ebf269-cca0-40e9-9a5a-2e046a4dc19b",
   "metadata": {},
   "source": [
    "Create row vectors `X_norm` and `Y_norm`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00727d02-b953-46a2-91a4-9fc7c00be578",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = adv_norm['TV']\n",
    "Y_norm = adv_norm['Sales']\n",
    "\n",
    "X_norm = np.array(X_norm).reshape((1, len(X_norm)))\n",
    "Y_norm = np.array(Y_norm).reshape((1, len(Y_norm)))\n",
    "\n",
    "print ('The shape of X_norm: ' + str(X_norm.shape))\n",
    "print ('The shape of Y_norm: ' + str(Y_norm.shape))\n",
    "print ('I have m = %d training examples!' % (X_norm.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483ecb1-f960-4421-945f-67eaa7d0deac",
   "metadata": {},
   "source": [
    "#### Define two variables for the neural net structure:\n",
    "- `n_x`: the size of the input layer\n",
    "- `n_y`: the size of the output layer\n",
    "\n",
    "using shapes of arrays `X` and `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fdae5e-8c62-480d-84b2-e084c5775414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    \n",
    "    return (n_x, n_y)\n",
    "\n",
    "(n_x, n_y) = layer_sizes(X_norm, Y_norm)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae7bbb-778c-449c-95af-342db11ef3f2",
   "metadata": {},
   "source": [
    "#### Initialize model parameters\n",
    "\n",
    "Implement the function `initialize_parameters()`, initializing the weights array of shape $(n_y \\times n_x) = (1 \\times 1)$ with random values and the bias vector of shape $(n_y \\times 1) = (1 \\times 1)$ with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b130e8-39ac-4678-b139-4b19c747e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W -- weight matrix of shape (n_y, n_x)\n",
    "                    b -- bias value set as a vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "        \n",
    "    W = np.random.randn(n_y, n_x) * 0.01\n",
    "    \n",
    "    \n",
    "    b = np.zeros((n_y, 1))\n",
    "    \n",
    "    \n",
    "    parameters = {\"W\": W,\n",
    "                  \"b\": b}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters(n_x, n_y)\n",
    "print(\"W = \" + str(parameters[\"W\"]))\n",
    "print(\"b = \" + str(parameters[\"b\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e7b75-6cd7-4051-a246-c78d4bc6e6e0",
   "metadata": {},
   "source": [
    "#### The loop!\n",
    "**Implement `forward_propagation()` (calculate the perceptron output) using the following the equation:**\n",
    "\n",
    "$Z =  w X + b,$\n",
    "\n",
    "$\\hat{Y} = Z$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f36c9-02c2-4713-9d78-dcfb902e828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    Y_hat -- The output\n",
    "    \"\"\"\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Forward Propagation to calculate Z.\n",
    "    Z = np.matmul(W, X) + b  #matrix multiplication of numpy arrays, plus bias, to obtain weighted sums! \n",
    "    Y_hat = Z\n",
    "\n",
    "    return Y_hat\n",
    "\n",
    "Y_hat = forward_propagation(X_norm, parameters)\n",
    "\n",
    "print(\"Some elements of output vector Y_hat:\", Y_hat[0, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72634f61-eff0-4e1d-ad58-18b91399d47f",
   "metadata": {},
   "source": [
    "**Quiz**: \n",
    "\n",
    "- What are the corresponding **Y** values? What are the errors/loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d4699-9b4d-431d-9fce-da90fd215190",
   "metadata": {},
   "source": [
    "**Define a cost function** \n",
    "\n",
    "Note that our weights were just initialized with some random values, so the model has not been trained yet.\n",
    "\n",
    "We define a cost function which will be used to train the model:\n",
    "\n",
    "$\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y_i} - y_i\\right)^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c850e-541a-4001-b1a7-a97a2653c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost function as a sum of squares\n",
    "    \n",
    "    Arguments:\n",
    "    Y_hat -- The output of the neural network of shape (n_y, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (n_y, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- sum of squares scaled by 1/(2*number of examples)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Number of examples.\n",
    "    m = Y_hat.shape[1]\n",
    "\n",
    "    # Compute the cost function.\n",
    "    cost = np.sum((Y_hat - Y)**2)/(2*m)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(Y_hat, Y_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab97f6-c18b-4264-afa5-4fa97aafb28c",
   "metadata": {},
   "source": [
    "**Implement `backward_propagation` function by calculating partial derivatives, as shown below:**\n",
    "\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L} }{ \\partial w } = \\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y_i} - y_i\\right)x_i,$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L} }{ \\partial b } = \\frac{1}{m}\\sum_{i=1}^{m} \\left(\\hat{y_i} - y_i\\right)$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45137edd-d533-4b92-a551-1451401b2e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(Y_hat, X, Y):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation, calculating gradients\n",
    "    \n",
    "    Arguments:\n",
    "    Y_hat -- the output of the neural network of shape (n_y, number of examples)\n",
    "    X -- input data of shape (n_x, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (n_y, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Backward propagation: calculate partial derivatives denoted as dW, db for simplicity. \n",
    "    dZ = Y_hat - Y\n",
    "    dW = 1/m * np.dot(dZ, X.T)\n",
    "    db = 1/m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW\": dW,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "grads = backward_propagation(Y_hat, X_norm, Y_norm)\n",
    "\n",
    "print(\"dW = \" + str(grads[\"dW\"]))\n",
    "print(\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5eda4-bdbe-4221-bd97-bcb6a36c8e74",
   "metadata": {},
   "source": [
    "**Update Parameters**: \n",
    "\n",
    "$w = w - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial w },$\n",
    "\n",
    "$b = b - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60050d-8a88-4a6b-9c5e-c9bacb340e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters \n",
    "    grads -- python dictionary containing gradients \n",
    "    learning_rate -- learning rate parameter for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\".\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\".\n",
    "    dW = grads[\"dW\"]\n",
    "    db = grads[\"db\"]\n",
    "    \n",
    "    # Update rule for each parameter.\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "    \n",
    "    parameters = {\"W\": W,\n",
    "                  \"b\": b}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters_updated = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W updated = \" + str(parameters_updated[\"W\"]))\n",
    "print(\"b updated = \" + str(parameters_updated[\"b\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dee659-db2e-481c-8042-3a22d4874647",
   "metadata": {},
   "source": [
    "**Create a function `nn_model()` to integrate neural net structure (shape), parameter initialization, and the loop (iterate: foward propagation, compute cost, backward propagation, and parameter updates)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adec91-9331-47da-87f2-b632442b2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, num_iterations=10, learning_rate=1.2, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (n_x, number of examples)\n",
    "    Y -- labels of shape (n_y, number of examples)\n",
    "    num_iterations -- number of iterations in the loop\n",
    "    learning_rate -- learning rate parameter for gradient descent\n",
    "    print_cost -- if True, print the cost every iteration\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[1]\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_y)\n",
    "    \n",
    "    # Loop\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"Y_hat\".\n",
    "        Y_hat = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"Y_hat, Y\". Outputs: \"cost\".\n",
    "        cost = compute_cost(Y_hat, Y)\n",
    "        \n",
    "        # Backpropagation. Inputs: \"Y_hat, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(Y_hat, X, Y)\n",
    "    \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads, learning_rate\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost every iteration.\n",
    "        if print_cost:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33390f75-61f7-4455-ac1b-6aee486f4031",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_simple = nn_model(X_norm, Y_norm, num_iterations=50, learning_rate=0.4, print_cost=True)\n",
    "print(\"W = \" + str(parameters_simple[\"W\"]))\n",
    "print(\"b = \" + str(parameters_simple[\"b\"]))\n",
    "\n",
    "W_simple = parameters[\"W\"]\n",
    "b_simple = parameters[\"b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6da8f-a3d2-45f5-8455-faf8e11a18a6",
   "metadata": {},
   "source": [
    "**Quiz**: \n",
    "\n",
    "- What happens when you increase the learning rate (*e.g* 4.4)? \n",
    "\n",
    "\n",
    "**Predictions** \n",
    "\n",
    "The final model parameters can be used for making predictions, but don't forget about normalization and denormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f0cfb-763a-43db-86c5-01db87075589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters, X_pred):\n",
    "    \n",
    "    # Retrieve each parameter from the dictionary \"parameters\".\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Use the same mean and standard deviation of the original training array X.\n",
    "    if isinstance(X, pd.Series):\n",
    "        X_mean = np.mean(X)\n",
    "        X_std = np.std(X)\n",
    "        X_pred_norm = ((X_pred - X_mean)/X_std).reshape((1, len(X_pred)))\n",
    "    else:\n",
    "        X_mean = np.array(np.mean(X)).reshape((len(X.axes[1]),1))\n",
    "        X_std = np.array(np.std(X)).reshape((len(X.axes[1]),1))\n",
    "        X_pred_norm = ((X_pred - X_mean)/X_std)\n",
    "    \n",
    "    \n",
    "    # Make predictions.\n",
    "    Y_pred_norm = np.matmul(W, X_pred_norm) + b\n",
    "    # Use the same mean and standard deviation of the original training array Y.\n",
    "    Y_pred = Y_pred_norm * np.std(Y) + np.mean(Y)\n",
    "    \n",
    "    return Y_pred[0]\n",
    "\n",
    "\n",
    "X_pred = np.array([50, 120, 280])\n",
    "Y_pred = predict(adv[\"TV\"], adv[\"Sales\"], parameters_simple, X_pred)\n",
    "print(f\"TV marketing expenses:\\n{X_pred}\")\n",
    "print(f\"Predictions of sales:\\n{Y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920891cb-d91c-45bd-bef6-74c0d47e031c",
   "metadata": {},
   "source": [
    "**Visualize the fitted model on data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27047046-0524-438a-ab43-78bb72d6b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(adv[\"TV\"], adv[\"Sales\"], color=\"black\")\n",
    "\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "    \n",
    "X_line = np.arange(np.min(adv[\"TV\"]),np.max(adv[\"TV\"])*1.1, 0.1)\n",
    "Y_line = predict(adv[\"TV\"], adv[\"Sales\"], parameters_simple, X_line)\n",
    "ax.plot(X_line, Y_line, \"r\")\n",
    "ax.plot(X_pred, Y_pred, \"bo\")\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11d2a3-4537-441e-af63-d4acea0ec979",
   "metadata": {},
   "source": [
    "### Neural Network Model with a Single Perceptron and Two Input Nodes\n",
    "\n",
    "We can write a multiple linear regression model with two independent variables $x_1$, $x_2$ as\n",
    "\n",
    "$\\hat{y} = w_1x_1 + w_2x_2 + b = Wx + b,\\tag{7}$\n",
    "\n",
    "where $Wx$ is the dot product of the input vector $x = \\begin{bmatrix} x_1 & x_2\\end{bmatrix}$ and the parameters vector $W = \\begin{bmatrix} w_1 & w_2\\end{bmatrix}$, scalar parameter $b$ is the intercept. The goal of the training process is to find the \"best\" parameters $w_1$, $w_2$ and $b$ such that the differences between original values $y_i$ and predicted values $\\hat{y}_i$ are minimum for the given training examples.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To describe the multiple regression problem, you can still use a model with one perceptron, but this time you need two input nodes, as shown in the following scheme:\n",
    "\n",
    "<img src=\"images/nn_model_linear_regression_multiple.png\" style=\"width:420px;\">\n",
    "\n",
    "The perceptron output calculation for every training example $x^{(i)} = \\begin{bmatrix} x_1^{(i)} & x_2^{(i)}\\end{bmatrix}$ can be written with dot product:\n",
    "\n",
    "$z^{(i)} = w_1x_1^{(i)} + w_2x_2^{(i)} + b = Wx^{(i)} + b,\\tag{8}$\n",
    "\n",
    "where weights are in the vector $W = \\begin{bmatrix} w_1 & w_2\\end{bmatrix}$ and bias $b$ is a scalar. The output layer will have the same single node $\\hat{y} = z$.\n",
    "\n",
    "Organise all training examples in a matrix $X$ of a shape ($2 \\times m$), putting $x_1^{(i)}$ and $x_2^{(i)}$ into columns. Then matrix multiplication of $W$ ($1 \\times 2$) and $X$ ($2 \\times m$) will give a ($1 \\times m$) vector\n",
    "\n",
    "$$WX = \n",
    "\\begin{bmatrix} w_1 & w_2\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "x_1^{(1)} & x_1^{(2)} & \\dots & x_1^{(m)} \\\\ \n",
    "x_2^{(1)} & x_2^{(2)} & \\dots & x_2^{(m)} \\\\ \\end{bmatrix}\n",
    "=\\begin{bmatrix} \n",
    "w_1x_1^{(1)} + w_2x_2^{(1)} & \n",
    "w_1x_1^{(2)} + w_2x_2^{(2)} & \\dots & \n",
    "w_1x_1^{(m)} + w_2x_2^{(m)}\\end{bmatrix}.$$\n",
    "\n",
    "And the model can be written as\n",
    "\n",
    "\n",
    "$Z =  W X + b,$\n",
    "\n",
    "$\\hat{Y} = Z $\n",
    "\n",
    "where $b$ is broadcasted to the vector of size ($1 \\times m$). These are the calculations to perform in the forward propagation step. Cost function will remain the same:\n",
    "\n",
    "$\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y_i} - y_i\\right)^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c652e001-01b6-48f2-b439-96aba42a497b",
   "metadata": {},
   "source": [
    "### Let's Dive Deep! An Introduction to Deep Learning \n",
    "\n",
    "Deep learning is a specific subfield of machine learning that involves learning representations\n",
    "from data with emphasis on learning successive layers of increasingly meaningful representations. The **deep** in **deep learning** stands for this idea of successive layers of representations. These layered representations are learned via models called neural\n",
    "networks, of whom perceptrons are building blocks. \n",
    "\n",
    "Below is a representation of a two-layered dense neural network, with the aforementioned processes of forward and backward propagation shown. \n",
    "\n",
    "<img src=\"images/dnn.png\" style=\"width:720px;\">\n",
    "\n",
    "\n",
    "#### Research question\n",
    "We want to build a model that can accurately predict the median house value in California districts, given a number of features from these districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e5f66e-2d92-4f68-b118-73be0fb13f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Load California housing dataset\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5375a1-6417-4323-aaeb-12c4603dcfbb",
   "metadata": {},
   "source": [
    "This is a dataset of 20,640 samples, with 8 features each.\n",
    "The target value is the `MedHouseVal`: Median House Value (($100k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5b891-3514-420d-b40d-1152976a8a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataset as data frame, for visualization\n",
    "df = fetch_california_housing(as_frame=True)['frame']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cdc482-e3d4-4287-9397-0e2d47151623",
   "metadata": {},
   "source": [
    "**Create Training, validation, and test datasets**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eed1ee-dc56-4359-acbf-7fa1160d8899",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create train, validation and test sets\n",
    "#training and test data\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "#split original train to train and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbea21c-7247-4949-bde9-88f3c30869ac",
   "metadata": {},
   "source": [
    "**Scale and normalize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f123532-6b52-4e8d-ae2e-18d74f7a6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_valid = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47c502-d62b-413d-a3c9-9d4e721b2e1f",
   "metadata": {},
   "source": [
    "**Building and training the model**\n",
    "\n",
    "We define a dense model with three intermediate layers, each 64 units. The model ends with a single unit and no activation (it will be a linear layer).\n",
    "\n",
    "The intermediate layers use a rectified linear unit (`relu`, zeroes out negative values) as their activation function.\n",
    "\n",
    "These are the commonly used activations functions:\n",
    "\n",
    "<img src=\"images/activation_functions.ppm.png\" style=\"width:620px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0297f-5f8d-4b29-9937-2b0239b708bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model: 3-layered, dense neural net\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1) # Single output neuron for continuous output, no activation\n",
    "])\n",
    "# Compile model\n",
    "model.compile(optimizer='sgd',loss='mse',metrics=['mse', 'mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cf04b-cf1d-4787-bc9d-0bec48071a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train (with validation at each epoch)\n",
    "#Notice Verbose set to zero!\n",
    "model.fit(X_train, y_train, epochs=100,validation_data=(X_valid, y_valid), verbose=0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d633ac-b154-41ad-8cbd-a6b4ea6ce66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training performance\n",
    "pd.DataFrame(model.history.history)[['loss','val_loss','mse','val_mse' ]].plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c8d7a-1a84-4464-bf03-d91f3f7d7543",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history)[['mae', 'val_mae']].plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195006a5-1cca-4cf6-ac01-3c8aefa30d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating predictions on new data\n",
    "Y_pred=model.predict(X_test)\n",
    "Y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b8547-f87e-4101-b715-38fe7206a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Evaluate model: how does the model perform on unseen data? \n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984f565-386b-46ea-a485-a5593d0b7c49",
   "metadata": {},
   "source": [
    "**Quiz**\n",
    "\n",
    "- Increase number of epochs, what's the minimum loss (mse) obtained?  \n",
    "- Change number of layers, and units per layer. How does this affect model accuracy? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
